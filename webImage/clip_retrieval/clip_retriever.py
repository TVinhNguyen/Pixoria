import torch
import clip
import numpy as np
import pickle
import faiss
from PIL import Image
import requests
from io import BytesIO

class CLIPRetriever:
    def __init__(self, index_path=None, mapping_path=None, use_gpu=True):
        """
        Khá»Ÿi táº¡o CLIP Retriever cho tÃ¬m kiáº¿m áº£nh báº±ng vÄƒn báº£n.
        
        Args:
            index_path (str): ÄÆ°á»ng dáº«n Ä‘áº¿n FAISS index.
            mapping_path (str): ÄÆ°á»ng dáº«n Ä‘áº¿n file mapping.
            use_gpu (bool): Sá»­ dá»¥ng GPU náº¿u cÃ³.
        """
        self.device = torch.device("cuda" if torch.cuda.is_available() and use_gpu else "cpu")
        
        # Táº£i model CLIP ViT-B/32
        print(f"ğŸ”„ Loading CLIP ViT-B/32 model for text retrieval...")
        self.model, self.preprocess = clip.load("ViT-B/32", device=self.device)
        self.model.eval()
        print(f"âœ… CLIP ViT-B/32 model loaded successfully")
        
        # Táº£i index vÃ  mapping náº¿u Ä‘Æ°á»£c cung cáº¥p
        self.index = None
        self.image_data = []
        
        if index_path and mapping_path:
            self.load(index_path, mapping_path, use_gpu)
        
    def load(self, index_path, mapping_path, use_gpu=True):
        """
        Táº£i FAISS index vÃ  mapping tá»« file.
        
        Args:
            index_path (str): ÄÆ°á»ng dáº«n file FAISS index.
            mapping_path (str): ÄÆ°á»ng dáº«n file mapping.
            use_gpu (bool): Sá»­ dá»¥ng GPU náº¿u cÃ³.
        """
        self.index = faiss.read_index(str(index_path))

        if use_gpu and faiss.get_num_gpus() > 0:
            res = faiss.StandardGpuResources()
            self.index = faiss.index_cpu_to_gpu(res, 0, self.index)

        with open(mapping_path, 'rb') as f:
            self.image_data = pickle.load(f)

        print(f"âœ… Loaded index with {self.index.ntotal} vectors")
        print(f"ğŸ“‚ Loaded {len(self.image_data)} image records")
    
    def encode_text(self, text):
        """
        MÃ£ hÃ³a vÄƒn báº£n thÃ nh vector Ä‘áº·c trÆ°ng sá»­ dá»¥ng CLIP.
        
        Args:
            text (str): VÄƒn báº£n cáº§n mÃ£ hÃ³a.
            
        Returns:
            np.ndarray: Vector Ä‘áº·c trÆ°ng cá»§a vÄƒn báº£n.
        """
        with torch.no_grad():
            text_inputs = clip.tokenize([text]).to(self.device)
            text_features = self.model.encode_text(text_inputs)
            text_features = text_features / text_features.norm(dim=-1, keepdim=True)
        
        return text_features.cpu().numpy()
    
    def retrieve_images(self, text_query, top_k=10):
        """
        TÃ¬m kiáº¿m áº£nh theo vÄƒn báº£n.
        
        Args:
            text_query (str): VÄƒn báº£n truy váº¥n.
            top_k (int): Sá»‘ lÆ°á»£ng káº¿t quáº£ tráº£ vá».
            
        Returns:
            list: Danh sÃ¡ch dict chá»©a thÃ´ng tin áº£nh tÆ°Æ¡ng tá»± vÃ  Ä‘iá»ƒm tÆ°Æ¡ng Ä‘á»“ng.
        """
        if self.index is None:
            print("âš ï¸ Index chÆ°a Ä‘Æ°á»£c táº£i. Vui lÃ²ng táº£i index trÆ°á»›c khi tÃ¬m kiáº¿m.")
            return []
        
        # MÃ£ hÃ³a vÄƒn báº£n thÃ nh vector
        text_features = self.encode_text(text_query)
        
        # TÃ¬m kiáº¿m áº£nh tÆ°Æ¡ng tá»±
        similarities, indices = self.index.search(text_features, top_k)
        
        # Táº¡o káº¿t quáº£
        results = []
        for i, idx in enumerate(indices[0]):
            if idx < len(self.image_data) and idx >= 0:
                if isinstance(self.image_data[idx], dict):
                    result = self.image_data[idx].copy()
                    result["similarity_score"] = float(similarities[0][i])
                else:
                    image_data = self.image_data[idx]
                    result = {
                        "id": idx,
                        "file": image_data if isinstance(image_data, str) else str(image_data),
                        "similarity_score": float(similarities[0][i])
                    }
                results.append(result)
                
        return results
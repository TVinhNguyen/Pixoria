import os
import json
import faiss
import numpy as np
import pickle
from tqdm import tqdm
import requests
from .feature_extractor import FeatureExtractor

class IndexBuilder:
    def __init__(self, feature_dim=2048, use_gpu=True):
        """
        X√¢y d·ª±ng FAISS index.
        
        Args:
            feature_dim (int): S·ªë chi·ªÅu c·ªßa vector ƒë·∫∑c tr∆∞ng.
            use_gpu (bool): S·ª≠ d·ª•ng GPU n·∫øu c√≥.
        """
        self.feature_dim = feature_dim
        self.use_gpu = use_gpu
        self.extractor = FeatureExtractor(use_gpu=use_gpu)
        self.image_data = []  # L∆∞u th√¥ng tin ·∫£nh

        # Kh·ªüi t·∫°o FAISS index v·ªõi L2 distance
        self.index = faiss.IndexFlatL2(feature_dim)

        # N·∫øu c√≥ GPU, chuy·ªÉn index sang GPU
        if use_gpu and faiss.get_num_gpus() > 0:
            print(f"üöÄ Using FAISS GPU (gpus available: {faiss.get_num_gpus()})")
            res = faiss.StandardGpuResources()
            self.index = faiss.index_cpu_to_gpu(res, 0, self.index)
        else:
            print("üñ•Ô∏è Using FAISS CPU")

    def load_s3_data(self, json_file=None, api_url=None, total_pages=None):
        """
        T·∫£i d·ªØ li·ªáu ·∫£nh t·ª´ file JSON ho·∫∑c t·ª´ API.
        
        Args:
            json_file (str): ƒê∆∞·ªùng d·∫´n ƒë·∫øn file JSON ch·ª©a th√¥ng tin ·∫£nh
            api_url (str): URL API c∆° s·ªü ƒë·ªÉ l·∫•y th√¥ng tin ·∫£nh
            total_pages (int): T·ªïng s·ªë trang d·ªØ li·ªáu (n·∫øu bi·∫øt)
            
        Returns:
            list: Danh s√°ch th√¥ng tin ·∫£nh
        """
        if json_file:
            with open(json_file, 'r') as f:
                data = json.load(f)
                if isinstance(data, dict) and "results" in data:
                    self.image_data = data["results"]
                else:
                    self.image_data = data
        elif api_url:
            base_url = api_url.split('?')[0] if '?' in api_url else api_url
            base_url = base_url.rstrip('/')
            
            if total_pages is None:
                # L·∫•y trang ƒë·∫ßu ti√™n ƒë·ªÉ ki·ªÉm tra s·ªë trang
                response = requests.get(f"{base_url}/?page=1")
                response.raise_for_status()
                
                try:
                    data = response.json()
                    
                    # Ki·ªÉm tra c·∫•u tr√∫c d·ªØ li·ªáu tr·∫£ v·ªÅ
                    print(f"Ki·ªÉm tra c·∫•u tr√∫c d·ªØ li·ªáu API tr·∫£ v·ªÅ: {type(data)}")
                    if isinstance(data, list):
                        # API tr·∫£ v·ªÅ tr·ª±c ti·∫øp danh s√°ch c√°c m·ª•c
                        page_data = data
                        print(f"API tr·∫£ v·ªÅ danh s√°ch v·ªõi {len(page_data)} m·ª•c")
                        if page_data and isinstance(page_data[0], str):
                            print("ƒê·ªãnh d·∫°ng: Danh s√°ch c√°c URL chu·ªói")
                        else:
                            print(f"M·∫´u d·ªØ li·ªáu ƒë·∫ßu ti√™n: {page_data[0] if page_data else None}")
                    elif isinstance(data, dict) and "results" in data:
                        page_data = data["results"]
                        total_count = data.get("count", 0)
                        print(f"T·ªïng s·ªë ·∫£nh: {total_count}")
                    else:
                        page_data = [data]  # ƒê·∫∑t trong m·ªôt danh s√°ch n·∫øu kh√¥ng kh·ªõp ƒë·ªãnh d·∫°ng d·ª± ki·∫øn
                    
                    self.image_data.extend(page_data)
                    print(f"üìÑ Loaded page 1 with {len(page_data)} items")
                    
                    # N·∫øu API tr·∫£ v·ªÅ theo ƒë·ªãnh d·∫°ng ph√¢n trang
                    next_url = data.get("next") if isinstance(data, dict) else None
                    page = 2
                    
                    while next_url:
                        try:
                            response = requests.get(next_url)
                            response.raise_for_status()
                            
                            data = response.json()
                            if isinstance(data, dict) and "results" in data:
                                page_data = data["results"]
                                next_url = data.get("next")
                            else:
                                page_data = data if isinstance(data, list) else [data]
                                next_url = None
                            
                            if not page_data:  # N·∫øu kh√¥ng c√≥ d·ªØ li·ªáu, ng·ª´ng
                                break
                                
                            self.image_data.extend(page_data)
                            print(f"üìÑ Loaded page {page} with {len(page_data)} items")
                            page += 1
                        except Exception as e:
                            print(f"‚ùå Error loading page {page}: {e}")
                            break
                
                except Exception as e:
                    print(f"‚ùå Error parsing API response: {e}")
                    # Th·ª≠ x·ª≠ l√Ω d∆∞·ªõi d·∫°ng vƒÉn b·∫£n
                    try:
                        text_data = response.text
                        lines = [line.strip() for line in text_data.split('\n') if line.strip()]
                        print(f"API tr·∫£ v·ªÅ {len(lines)} d√≤ng vƒÉn b·∫£n")
                        self.image_data = lines
                    except Exception as text_e:
                        print(f"‚ùå Kh√¥ng th·ªÉ x·ª≠ l√Ω ph·∫£n h·ªìi d∆∞·ªõi d·∫°ng vƒÉn b·∫£n: {text_e}")
            
            else:
                # N·∫øu bi·∫øt tr∆∞·ªõc s·ªë trang, s·ª≠ d·ª•ng tqdm ƒë·ªÉ theo d√µi ti·∫øn tr√¨nh
                for page in tqdm(range(1, total_pages + 1), desc="T·∫£i d·ªØ li·ªáu"):
                    try:
                        response = requests.get(f"{base_url}/?page={page}")
                        response.raise_for_status()
                        
                        try:
                            data = response.json()
                            if isinstance(data, list):
                                page_data = data
                            elif isinstance(data, dict) and "results" in data:
                                page_data = data["results"]
                            else:
                                page_data = [data]
                                    
                            self.image_data.extend(page_data)
                            
                        except Exception as e:
                            print(f"‚ùå Error parsing API response on page {page}: {e}")
                            # Th·ª≠ x·ª≠ l√Ω d∆∞·ªõi d·∫°ng vƒÉn b·∫£n
                            try:
                                text_data = response.text
                                lines = [line.strip() for line in text_data.split('\n') if line.strip()]
                                self.image_data.extend(lines)
                            except Exception as text_e:
                                print(f"‚ùå Kh√¥ng th·ªÉ x·ª≠ l√Ω ph·∫£n h·ªìi d∆∞·ªõi d·∫°ng vƒÉn b·∫£n: {text_e}")
                                
                    except Exception as e:
                        print(f"‚ùå Error loading page {page}: {e}")
        
        else:
            raise ValueError("C·∫ßn cung c·∫•p json_file ho·∫∑c api_url")
            
        print(f"üì∏ Loaded {len(self.image_data)} items t·ªïng c·ªông")
        
        # Ki·ªÉm tra v√† in m·ªôt s·ªë m·∫´u d·ªØ li·ªáu
        if self.image_data:
            print("M·∫´u d·ªØ li·ªáu:")
            for i in range(min(3, len(self.image_data))):
                print(f"  - {self.image_data[i]}")
        
        return self.image_data

    def build_index_from_s3(self, json_file=None, api_url=None, total_pages=None, batch_size=32):
        """
        X√¢y d·ª±ng index FAISS t·ª´ d·ªØ li·ªáu ·∫£nh S3.
        
        Args:
            json_file (str): ƒê∆∞·ªùng d·∫´n ƒë·∫øn file JSON ch·ª©a th√¥ng tin ·∫£nh
            api_url (str): URL API ƒë·ªÉ l·∫•y th√¥ng tin ·∫£nh
            total_pages (int): T·ªïng s·ªë trang (n·∫øu bi·∫øt tr∆∞·ªõc)
            batch_size (int): K√≠ch th∆∞·ªõc batch khi x·ª≠ l√Ω ·∫£nh
        """
        # T·∫£i d·ªØ li·ªáu ·∫£nh
        self.load_s3_data(json_file, api_url, total_pages)
        print("üîç Extracting features...")

        # Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng
        features, valid_images = self.extractor.extract_features_from_s3_data(self.image_data, batch_size)
        
        # C·∫≠p nh·∫≠t danh s√°ch ·∫£nh v·ªõi ch·ªâ c√°c ·∫£nh h·ª£p l·ªá
        self.image_data = valid_images
        
        if len(features) > 0:
            print("‚ö° Adding features to FAISS index...")
            self.index.add(features)
            print(f"‚úÖ Index built with {self.index.ntotal} images")
        else:
            print("‚ùå No valid images found. Index not built.")

    def save(self, index_path, mapping_path):
        """
        L∆∞u FAISS index v√† mapping v√†o file.
        
        Args:
            index_path (str): ƒê∆∞·ªùng d·∫´n l∆∞u FAISS index.
            mapping_path (str): ƒê∆∞·ªùng d·∫´n l∆∞u mapping.
        """
        if self.use_gpu and faiss.get_num_gpus() > 0:
            faiss.write_index(faiss.index_gpu_to_cpu(self.index), str(index_path))
        else:
            faiss.write_index(self.index, index_path)

        with open(mapping_path, 'wb') as f:
            pickle.dump(self.image_data, f)

        print(f"üíæ Index saved to {index_path}")
        print(f"üìù Image mapping saved to {mapping_path}")

    def load(self, index_path, mapping_path):
        """
        T·∫£i FAISS index v√† mapping t·ª´ file.
        
        Args:
            index_path (str): ƒê∆∞·ªùng d·∫´n file FAISS index.
            mapping_path (str): ƒê∆∞·ªùng d·∫´n file mapping.
        """
        self.index = faiss.read_index(str(index_path))

        if self.use_gpu and faiss.get_num_gpus() > 0:
            res = faiss.StandardGpuResources()
            self.index = faiss.index_cpu_to_gpu(res, 0, self.index)

        with open(mapping_path, 'rb') as f:
            self.image_data = pickle.load(f)

        print(f"‚úÖ Loaded index with {self.index.ntotal} vectors")
        print(f"üìÇ Loaded {len(self.image_data)} image records")

    def search(self, query_image, top_k=5):
        """
        T√¨m ki·∫øm ·∫£nh t∆∞∆°ng t·ª±.
        
        Args:
            query_image (str): URL ho·∫∑c ƒë∆∞·ªùng d·∫´n ·∫£nh truy v·∫•n.
            top_k (int): S·ªë l∆∞·ª£ng k·∫øt qu·∫£ tr·∫£ v·ªÅ.
            
        Returns:
            list: Danh s√°ch dict ch·ª©a th√¥ng tin ·∫£nh t∆∞∆°ng t·ª± v√† kho·∫£ng c√°ch.
        """
        # Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng t·ª´ ·∫£nh truy v·∫•n
        query_feature = self.extractor.extract_features(query_image)
        if query_feature is None:
            return []
            
        # Reshape query_feature n·∫øu c·∫ßn
        query_feature = query_feature.reshape(1, -1)
            
        # T√¨m ki·∫øm ·∫£nh t∆∞∆°ng t·ª±
        distances, indices = self.index.search(query_feature, top_k)
        
        # T·∫°o k·∫øt qu·∫£
        results = []
        for i, idx in enumerate(indices[0]):
            if idx < len(self.image_data) and idx >= 0:  # Ki·ªÉm tra index h·ª£p l·ªá
                # X·ª≠ l√Ω c·∫£ tr∆∞·ªùng h·ª£p self.image_data[idx] l√† string ho·∫∑c dict
                if isinstance(self.image_data[idx], dict):
                    result = self.image_data[idx].copy()  # Copy ƒë·ªÉ kh√¥ng thay ƒë·ªïi d·ªØ li·ªáu g·ªëc
                    result["distance"] = float(distances[0][i])
                else:
                    # N·∫øu kh√¥ng ph·∫£i dict (c√≥ th·ªÉ l√† string), t·∫°o dict m·ªõi
                    image_data = self.image_data[idx]
                    result = {
                        "id": idx,  # S·ª≠ d·ª•ng index l√†m id n·∫øu kh√¥ng c√≥
                        "file": image_data if isinstance(image_data, str) else str(image_data),
                        "distance": float(distances[0][i])
                    }
                results.append(result)
                
        return results